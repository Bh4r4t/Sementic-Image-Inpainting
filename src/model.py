import torch
import torch.nn as nn
import torch.nn.functional as F


class Generator(nn.Module):
    def __init__(self, nz, features, out_channel):
        """
        Args:
            nz: size of latent space vector
            features: parameter
            out_channer: number of output channel in image generated by generator
        """
        super(Generator, self).__init__()
        self.conv1 = nn.ConvTranspose2d(nz, features*16, 4, 1, padding=0, bias=False)
        # ouput dimension -> (features*16) * 4*4
        self.BN1 = nn.BatchNorm2d(features*16)
        
        self.conv2 = nn.ConvTranspose2d(features*16, features*8, 3, 1, padding = 0, bias=False)
        #output dimension -> (features*8)*6*6
        self.BN2 = nn.BatchNorm2d(features*8)

        self.conv3 = nn.ConvTranspose2d(features*8, features*4, 4, 2, padding=1, bias = False)
        #output dimension -> (features*4)*12*12
        self.BN3 = nn.BatchNorm2d(features*4)

        self.conv4 = nn.ConvTranspose2d(features*4, features*4, 4, 2, padding=1, bias = False)
        #output dimension -> (features*4)*24*24
        self.BN4 = nn.BatchNorm2d(features*4)        
        
        self.conv5 = nn.ConvTranspose2d(features*4, features*2, 4, 2, padding=1, bias = False)
        #output dimension -> (features*2)*48*48
        self.BN5 = nn.BatchNorm2d(features*2)
        
        self.conv6 = nn.ConvTranspose2d(features*2, features, 4, 2, padding=1, bias=False)
        #output dimension -> features*96*96
        self.BN6 = nn.BatchNorm2d(features)
        
        self.conv7 = nn.Conv2d(features, out_channel, 3, 1, padding = 1, bias=False)
        #output dimension -> channels*96*96
        
    def forward(self, x):
        x = F.relu(self.BN1(self.conv1(x)))
        x = F.relu(self.BN2(self.conv2(x)))
        x = F.relu(self.BN3(self.conv3(x)))
        x = F.relu(self.BN4(self.conv4(x)))
        x = F.relu(self.BN5(self.conv5(x)))
        x = F.relu(self.BN6(self.conv6(x)))
        x = self.conv7(x)
        x = torch.tanh(x)
        return x
    
# normal weight initialization for generator network 
def init_weight(x):
    classname = x.__class__.__name__
    if classname.find('ConvTranspose2d') != -1:
        nn.init.normal_(x.weight.data, 0.0, 1.0)

    if classname.find('BatchNorm2d') != -1 :
        x.weight.data.uniform_(0.0, 1.0)
        

class Discriminator(nn.Module):
    def __init__(self, features):
        super(Discriminator, self).__init__()
        self.conv1 = nn.Conv2d(3, features, 4, 2, 1, bias=False)
        # output dimension -> (features)*48*48
        self.BN1 = nn.BatchNorm2d(features)

        self.conv2 = nn.Conv2d(features, features*2, 4, 2, 1, bias = False)
        # output dimension -> (features*2)*24*24
        self.BN2 = nn.BatchNorm2d(features*2)
        
        self.conv3 = nn.Conv2d(features*2, features*4, 5, 2, 1, bias = False)
        # output dimension -> (features*4)*11*11
        self.BN3 = nn.BatchNorm2d(features*4)
        
        self.conv4 = nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False)
        # output dimension -> (features*8)*5*5
        self.BN4 = nn.BatchNorm2d(features*8)
        
        self.conv5 = nn.Conv2d(features*8, 1, 4, 2, 0, bias=False)
        # output dimension -> 1*1*1        
        
    def forward(self, x):
        x = F.leaky_relu(self.conv1(x), 0.2)
        x = F.leaky_relu(self.BN2(self.conv2(x)), 0.2)
        x = F.leaky_relu(self.BN3(self.conv3(x)), 0.2)
        x = F.leaky_relu(self.BN4(self.conv4(x)), 0.2)
        x = torch.sigmoid(self.conv5(x))
        return x.view(-1, 1).squeeze(0)
